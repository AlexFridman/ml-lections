\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}
\usepackage{cancel}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}
\renewcommand{\CancelColor}{\color{red}}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок, Н. Поваров, Яндекс
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2013
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Линейные модели: введение\\\small{по материалам "The Elements of Statistical Learning"}}
\author[]{\small{%
И.~Куралёнок,
Н.~Поваров}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\normalsize {\color{red}Я}ндекс \\
\vspace{80pt}
\footnotesize СПб, 2013
\end{center}
\end{frame}

\section{Линейные модели}
\begin{frame}{Формальная постановка}
Ищем решающую функцию в виде:
$$
y = F(\lambda, x) = \lambda^T x
$$
Такое решение кажется примитивным!
\end{frame}
\begin{frame}{Формальная постановка}
Ищем решающую функцию в виде:
$$
y = F(x, \lambda) = \lambda^T x = x^T \lambda
$$
Такое решение кажется примитивным! \\
До того как мы расскажем что такое $x$.
\end{frame}
\begin{frame}{Какое $x$ бывает}
Просто фичи:
$$
x \in \mathbb{R}^n
$$
Мономы:
$$
u \in \mathbb{R}^n
x = \prod u_j
$$
Произвольные функции:
$$
u \in \mathbb{R}^n
x: \mathbb{R}^n \to \mathbb{R}
$$
В любом случае мы всегда можем посчитать значение $x$ по входным параметрам.
\end{frame}

\section{Линейная регрессия}
\begin{frame}{Простое решение}
$$
\arg \min_{\lambda} \|F(X, \lambda) - y\| = \arg \min_{\lambda} \|X \lambda - y\|
$$
Если норма $l_2$, то:
$$
{\partial T \over \partial X} = 2 X^T\left(X\lambda - y\right) = 0
$$
$$
\lambda_0 = (X^TX)^{-1}X^Ty
$$
\end{frame}

\begin{frame}{Геометрическая интерпретация}
Если посмотреть на колонки, соответствующие фичам то картинка такая:
\begin{center}
\includegraphics[height=0.5\textheight]{3_2.png}
\end{center}
Об этом говорит (если нам все удалось):
$$
X^T(y - \hat{y}) = X^T(y - X\lambda_0) = 0
$$
В случае, если $rank(X) < n$ ортогональность остается!
\end{frame}

\subsection{Статистические свойства решения}
\begin{frame}{Статистические свойства решения}
Если наблюдения независимы, $Var(y) = const$, а $x$ вычислены точно:
$$
Var(\lambda) = \left(X^TX\right)^{-1}\frac{1}{n - m - 1}\|y - \hat{y}\|_2
$$
А если еще и предположить, что $y=\lambda_1^Tx + \epsilon$ и $\epsilon \sim N(0,\sigma)$:
$$
\lambda_0 \sim N(\lambda_1, \left(X^TX\right)^{-1}\sigma^2)
$$
а наблюдаемая $\sigma$ для $y$ распределена по $\chi^2$:
$$
(n-m-1)\hat{\sigma} = \|y - \hat{y}\|_2 \sim \sigma \chi^2_{n-m-1}
$$
\end{frame}

\begin{frame}{А точно $\lambda_{0_i} \ne 0$?}
Введем такую штуку ($Z$-score):
$$
z_i = \frac{\lambda_{0_i}}{\hat{\sigma}\sqrt{v_i}}
$$
где $v_i$ --- диагональный элемент $\left(X^TX\right)^{-1}$. Если подумать что $\lambda_{0_i} = 0$, то:
$$
z_i \sim T_{n-m-1}
$$
Чем больше $Z$-score, тем более мы уверены, что $\lambda_{0_i} \ne 0$
\end{frame}

\subsection{Теорема Гаусса-Маркова}
\begin{frame}{Теорема Гаусса-Маркова}
\begin{theorem}[]
Линейное приближение по MSE обладает на наименьшим разбросом из всех несмещенных линейных решений
\end{theorem}
\begin{itemize}
\item[$\Rightarrow$] для того, чтобы сделать решение более стабильным надо вводить bias
\item[$\Rightarrow$] простым MSE нам не отделаться, надо будет менять $T$
\end{itemize}
\end{frame}

\subsection{Расширение на несколько целевых функций}
\begin{frame}{Расширение на несколько целей}
$$
y_i \in \mathbb{R}^k
$$
В этом случае задача превращается в такую:
$$
\arg \min_\Lambda tr\left((Y - X\Lambda)^T(Y - X\Lambda)\right)
$$
$$
\Lambda_0 = \left(X^TX\right)^{-1} X^TY
$$
Если же $y = x^T\Lambda + \epsilon$, $\epsilon \sim N(0, \Sigma)$:
$$
\arg \min_\Lambda \left((Y - X\Lambda)^T\Sigma^{-1}(Y - X\Lambda)\right)
$$
\end{frame}

\section{Линейная классификация}
\begin{frame}{Классификация}
$$
x \in \mathbb{R}^n, y \in \{1,\ldots, k\}
$$
Можем пойти по-простому и решить регрессией:
$$
\gamma_{ij} = \left\{\begin{array}{ll}
1,& i = y_i \\
0 
\end{array}\right.
$$
\end{frame}

\subsection{Простая линейная модель и ее вариации}
\subsection{Линейный дискриминантный анализ (LDA)}
\subsection{Логистическая регрессия}
\section{Бонус: метрики классификации}

\section{Домашнее задание}
\begin{frame}{Результаты ДЗ второй недели}
\scriptsize
\begin{center}
\begin{enumerate}
\item 6af9df
\item dccc3e
\item f33f66
\item f1015e
\item f33f2d
\item 458048
\item 93184e
\item 608072
\item 824e76
\item 88d593
\item cfd271
\item 48364c
\item c5b930
\item 080c07
\item 579569
\item 01b988
\item 68c819
\item dcb652
\item ba605a
\item 692f0b
\item 6aca1b
\end{enumerate}
%Не отличаются 1-2 место, 3-6, 8-15, 16-21
\end{center}
\end{frame}
\begin{frame}{Домашнее задание}
\begin{itemize}
\item SVN, howto.txt
\item Две недели
\end{itemize}
\end{frame}
\end{document}
