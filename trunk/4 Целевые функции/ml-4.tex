\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}
\usepackage{cancel}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}
\renewcommand{\CancelColor}{\color{red}}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок, Н. Поваров, Яндекс
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2013
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Машинное обучение: обзор целевых функций\\\small{}}
\author[]{\small{%
И.~Куралёнок,
Н.~Поваров}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\normalsize {\color{red}Я}ндекс \\
\vspace{80pt}
\footnotesize СПб, 2013
\end{center}
\end{frame}

\section{Введение в проблематику и классификация целевых функций}
\begin{frame}{Задача на сегодня}
\textit{Строить варианты целевой функции на заданную тему.}\\
~\\
Для этогого нам понадобится:
\begin{itemize}
  \item узнать чем отличается измерение от оптимизации;
  \item понять какие существуют подходы к построению целевой функции;
  \item научиться строить целевые функции для заданных примеров (это уже ДЗ).
\end{itemize}
\end{frame}

\begin{frame}{Пример}
Вахтер хочет понять кого пускать в парадную. Он хочет минимизировать свою работу (больше спать) по:
\begin{itemize}
   \item проверке входящих;
   \item разборкам с жильцами/руководством;
   \item уборке/проветриванию.
\end{itemize}
Для этого ему надо проверять входящих (думать).
Однако, минимизировать ``время сна'' напрямую очень сложно. Наша задача помочь бедному вахтеру.
\end{frame}

\begin{frame}{Суть проблемы}
Если мы понимаем чего хотим: $\mathcal{M}(F_0)(X)$ (линейка позволяющая измерить конкретное решение), то задачу оптимизации можно переписать так:
$$
\max_{\mathcal{T}} \mathcal{M} \left(\arg\max_{F} \mathcal{T}(F, L)\right)(T)
$$
Если выборка не смещена по параметрам оптимизации, то К.О. говорит нам:
$$
\mathcal{M} \equiv \arg \max_{\mathcal{T}} \mathcal{M}\left(\arg\max_{F} \mathcal{T}(F, L)\right)(T)
$$
Однако, все не так просто.
\end{frame}

\begin{frame}{Про вахтера в новых обозначениях}
\begin{description}
  \item[$\mathcal{M}$] --- время сна;
  \item[$F$] --- способы проверки входящих;
  \item[$\mathcal{T}$] --- способы оценить проверку входящих.
\end{description}
Например в качестве $F$ может выступать оценка вероятности того, что ``клиент'' --- проблемный. Тогда $\mathcal{T}$ может быть как собственно время сна, так и средняя ошибка предсказания по результату работы функции $F$.
\end{frame}

\begin{frame}{Проблема в построении}
Что может быть ``не так'' в очевидном решении:
\begin{itemize}
  \item $\mathcal{M}$ может быть неудобна для оптимизации (кусочно-постоянная, например);
  \item сложно гарантировать несмещенность по параметрам оптимизации;
  \item сложно собирать данные в терминах $\mathcal{M}$;
\end{itemize} 
Поэтому все еще актуально решать исходную задачу:
$$
\max_{\mathcal{T}} \mathcal{M} \left(\arg\max_{F} \mathcal{T}(F, L)\right)(T)
$$
\end{frame}

\begin{frame}{Как можно подойти к построению $\mathcal{T}$}
Можно исходить из трех соображений:
\begin{description}
\small
\leftmargin=-5em
\itemindent=0em
\labelwidth=0em
\leftskip=-5em
  \item[$\mathcal{T} \equiv \mathcal{M}$:] усреднение $\mathcal{M}$ по всему доступному опыту;
  \item[$\arg\max_{F} \mathcal{T}(F, L) = \arg\max_{F} \mathcal{M}(F, L)$:]~\\
  \begin{itemize}
  \itemindent=0em
  \leftskip=-4em
    \item регрессия по ``очкам'': введем для каждого наблюдения стоимость, и будем ее приближать по $\mathcal{T}$;
    \item принцип максимальной энтропии;
    \item принцип минимального описания;
  \end{itemize}
  \item[$\max_{\mathcal{T}} \mathcal{M} \left(\arg\max_{F} \mathcal{T}(F, L)\right)(T)$:] вероятностное моделирование происходящего: как можно получить $\mathcal{M}$ из удобного $\mathcal{T}$.
\end{description} 
\end{frame}

\section{Способы усреднения}

\begin{frame}{Средние значения}
Будем оценивать каждое наблюдение. Хотим улучшить результат в среднем.
$$
F_0 = \arg \max_F \frac{1}{n} \sum_i \mathfrak{m}(F(x_i),y_i)
$$
Поделили большую $\mathcal{M}$ на много маленьких $\mathfrak{m}$. 
\begin{description}
\small
\leftmargin=-5em
\itemindent=0em
\labelwidth=0em
\leftskip=-5em
  \item[\color{green}+] по наблюдениям делить естественно;
  \item[\color{red}---] надо следить за независимостью наблюдений;
  \item[\color{red}---] работает только для ситуаций когда нет $\infty$ потерь/приобретений;
\end{description} 
\end{frame}

\begin{frame}{Средние значения бывают разные}
Последнюю проблему можно решить с помощью других средних:
$$\begin{array}{l}
\sqrt[n]{\prod x_i} \\
2ab \over a+b \\
\end{array}$$
\end{frame}

\begin{frame}{Разные только пространства усреднения}
$$
A(\{x_i\}) = f^{-1}\left(\frac{1}{n} \sum_i f(x_i)\right)
$$
В этих терминах все средние отличаются лишь отображением $f$;
\begin{description}
  \item[арифметическое]: $f(x) = x$; 
  \item[геометрическое]: $f(x) = \log x$; 
  \item[гармоническое]: $f(x) = \frac{1}{x}$; 
\end{description}
\end{frame}

\section{Очки и их учет}

\begin{frame}{Почему это работает}
Пусть задана метрика $\mathcal{M}$, хотим получить решение $F$, которое ее максимизирует. Введем дополнительную интерпретацию наблюдений $s(y_i) \in \mathbb{R}$. Будем предсказывать $s(y_i)$:
$$
F_0 = \arg\min_F \sum_i \|F(x) - s(y_i)\|_{l_q})
$$
\begin{itemize}
  \item формирование $s(y)$ --- отдельная проблема;
  \item регрессия известная задача;
  \item все гладко\footnote{для некоторых $q$ :)}, выпукло и удобно для оптимизации.
\end{itemize}
\end{frame}

\begin{frame}{Виды невязки $l_q$}
Невязку можно интерпретировать по-разному и в зависимости от интерпретации подбирать $q$:
$$
l_q(x,b) = \|x - b\|_{l_q} = \left(\sum_i\|x_i - b_i\|^q\right)^\fraq{1}{q}
$$
Все чуть менее гладко, чем хотелось бы, но и такие штуки оптимизируются с помощью односторонних градиентов.
\end{frame}

\begin{frame}{Экстремальные случаи $q$}
Особенно интересны экстремальные случаи $q$:
$$\begin{array}{l}
l_0(x,b) = \sum_i I\{x = b\} \\
l_{\infty}(x,b) = max |x - b| \\
\end{array}$$
Очень понятный физический смысл, но с гладкостью беда: оптимизация $l_0$ $NP$-hard.
\end{frame}

\begin{frame}{Как выглядят разные $q$}
\centering
\includegraphics[width=0.2\textwidth]{lq1.png}\includegraphics[width=0.65\textwidth]{lq2.png}
Часто из $l_0$ делают $l_1$
\end{frame}

\begin{frame}{Подбираем ``очки''} % повтор
\end{frame}

\begin{frame}{Вспоминая вахтера}
\end{frame}

\subsection{Вероятностные модели}

\begin{frame}{Моделирование вахтера}
Попробуем объяснить происходящее, зная как оно бывает.
\end{frame}

\begin{frame}{Оптимизация вероятностной модели}
\end{frame}

\begin{frame}{Почему это работает} % переход в общие термины
\end{frame}

\begin{frame}{Байесовские методы}
\end{frame}

\begin{frame}{Байесовские методы (практика)}
\end{frame}

\begin{frame}{Байесовские методы (свойства)}
\end{frame}

\begin{frame}{Maximum aposteriori}{(Байес по простому)}
\end{frame}

\begin{frame}{Метод максимального правдоподобия}{(Байес совсем по простому)}
\end{frame}

\begin{frame}{Веса при LL}
\end{frame}
\subsection{Математические свойства ММП}
\begin{frame}{Сходимость ММП}
\end{frame}
\begin{frame}{Асимптотическая нормальность ММП}
\end{frame}
\section{Принцип максимальной энтропии}

\begin{frame}{Принцип максимальной энтропии}
\begin{itemize}
  \item Выразим априорные свойства в виде ограничений;
  \item Найдём распределение обладающее максимальной энтропией;
  \item Когда хочется своего $p(x|I)$ решение будет другое.
\end{itemize}
$$\begin{array}{l}
  \sum_i p(x_i|I)f_k(x_i) = f_k^0, k = 1, ..., m \\
  \\
  p(x|I) = \frac{1}{Z}e^{\sum_k\lambda_kf_k(x)} \\
  \\
  Z = \sum_i\exp^{\sum_k\lambda_kf_k(x)} \\
  \\
  f_k^0 = \frac{\partial}{\partial\lambda_k}logZ
\end{array}$$
\end{frame}

\begin{frame}{Почему это работает} % переход в общие термины
\end{frame}

\section{Принцип наименьшего описания}

\begin{frame}{Принцип наименьшего описания}
\begin{itemize}
  \item Формализация бритвы Оккама;
  \item Колмогоров/Solomonoff;
  \item Вводим сложность по Колмогорову;
  \item Находим оптимальное решение; 
  \item По хорошему вероятность = 1.
\end{itemize}
$$
  F_0 = \arg\min_{F:p(X|F)\ge\epsilon} C(F)
$$

\end{frame}

\begin{frame}{Почему это работает} % переход в общие термины
\end{frame}

\section{Выводы}
\begin{frame}{Подводя итог}
\end{frame}

\begin{frame}{Сглаживание таргета}
\centering
Когда целевая функция "плохая":
$$
F_0 = \arg\max_F UT(X|F) = \arg\max_F \mu_{x \sim p(F)}(UT(x))
$$
\end{frame}

\begin{frame}{Чему следовать выбирая таргет}
\begin{itemize}
  \item Чувство прекрасного;
  \item Возможность применять математику:
  \begin{itemize}
    \item Скорость вычисления;
    \item Дифференцируемость (градиентные методы).
  \end{itemize}
  \item Наличие интересных внутренних параметров;
  \item Возможность проверить осмысленность промежуточных результатов.
\end{itemize}
\end{frame}

\begin{frame}{Про накопление знания в таргет функции}
\end{frame}

\section{ДЗ}
\begin{frame}{Задание на дом}
\begin{itemize}
  \item Придумать таргеты для некоторых задач; 
  \item Точные задачи приведены в файле howto.txt; 
  \item Дедлайн 18 октября.
\end{itemize}

\end{frame}
\end{document}
